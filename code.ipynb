{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0cZecOnhQXw"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.helpers import DataConnection\n",
        "from ibm_watsonx_ai.helpers import ContainerLocation\n",
        "\n",
        "training_data_references = [\n",
        "    DataConnection(\n",
        "        data_asset_id='ea2f5fd3-ef0c-449a-8584-9ce205006ccb'\n",
        "    ),\n",
        "]\n",
        "training_result_reference = DataConnection(\n",
        "    location=ContainerLocation(\n",
        "        path='auto_ml/58706bb2-a401-4e92-9ff7-d4333cd677b7/wml_data/1c48396f-7855-4e99-9e86-f26b20abd395/data/automl',\n",
        "        model_location='auto_ml/58706bb2-a401-4e92-9ff7-d4333cd677b7/wml_data/1c48396f-7855-4e99-9e86-f26b20abd395/data/automl/model.zip',\n",
        "        training_status='auto_ml/58706bb2-a401-4e92-9ff7-d4333cd677b7/wml_data/1c48396f-7855-4e99-9e86-f26b20abd395/training-status.json'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."
      ],
      "metadata": {
        "id": "GRgQoP7Lhaz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_metadata = dict(\n",
        "    prediction_type='multiclass',\n",
        "    prediction_column='PMGSY_SCHEME',\n",
        "    holdout_size=0.1,\n",
        "    scoring='accuracy',\n",
        "    csv_separator=',',\n",
        "    random_state=33,\n",
        "    max_number_of_estimators=2,\n",
        "    training_data_references=training_data_references,\n",
        "    training_result_reference=training_result_reference,\n",
        "    deployment_url='https://au-syd.ml.cloud.ibm.com',\n",
        "    project_id='de664104-b212-48ac-87e4-e28afe315fa4',\n",
        "    drop_duplicates=True,\n",
        "    include_batched_ensemble_estimators=['BatchedTreeEnsembleClassifier(ExtraTreesClassifier)', 'BatchedTreeEnsembleClassifier(LGBMClassifier)', 'BatchedTreeEnsembleClassifier(RandomForestClassifier)', 'BatchedTreeEnsembleClassifier(SnapBoostingMachineClassifier)', 'BatchedTreeEnsembleClassifier(SnapRandomForestClassifier)', 'BatchedTreeEnsembleClassifier(XGBClassifier)'],\n",
        "    classes=['PM-JANMAN', 'PMGSY-I', 'PMGSY-II', 'PMGSY-III', 'RCPLWEA'],\n",
        "    feature_selector_mode='auto'\n",
        ")"
      ],
      "metadata": {
        "id": "oGbQpqTAhjwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set n_jobs parameter to the number of available CPUs"
      ],
      "metadata": {
        "id": "pQ-v8rUThp5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, ast\n",
        "CPU_NUMBER = 4\n",
        "if 'RUNTIME_HARDWARE_SPEC' in os.environ:\n",
        "    CPU_NUMBER = int(ast.literal_eval(os.environ['RUNTIME_HARDWARE_SPEC'])['num_cpu'])"
      ],
      "metadata": {
        "id": "PGWRZdIth3sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "watsonx.ai connection\n",
        "This cell defines the credentials required to work with the watsonx.ai Runtime."
      ],
      "metadata": {
        "id": "GMB5SIz4h7kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "api_key = getpass.getpass(\"Please enter your api key (press enter): \")"
      ],
      "metadata": {
        "id": "iaR_-rNih_MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai import Credentials\n",
        "\n",
        "credentials = Credentials(\n",
        "    api_key=api_key,\n",
        "    url=experiment_metadata['deployment_url']\n",
        ")"
      ],
      "metadata": {
        "id": "ogn6XDy4iJRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai import APIClient\n",
        "\n",
        "client = APIClient(credentials)\n",
        "\n",
        "if 'space_id' in experiment_metadata:\n",
        "    client.set.default_space(experiment_metadata['space_id'])\n",
        "else:\n",
        "    client.set.default_project(experiment_metadata['project_id'])\n",
        "\n",
        "training_data_references[0].set_client(client)"
      ],
      "metadata": {
        "id": "TnZmrWYSiMR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incremental learning\n",
        "\n",
        "Get pipeline\n",
        "Download and save a pipeline model object from the AutoAI training job (lale pipeline type is used for inspection and partial_fit capabilities)."
      ],
      "metadata": {
        "id": "TzCSu_yIiTsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai.experiment import AutoAI\n",
        "\n",
        "pipeline_optimizer = AutoAI(credentials, project_id=experiment_metadata['project_id']).runs.get_optimizer(metadata=experiment_metadata)\n",
        "pipeline_model = pipeline_optimizer.get_pipeline(pipeline_name='Pipeline_10', astype='lale')"
      ],
      "metadata": {
        "id": "UFldBYu3ifB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader\n",
        "Create DataLoader iterator to retrieve training dataset in batches. DataLoader is Torch compatible (torch.utils.data), returning Pandas DataFrames.\n",
        "\n",
        "Note: If reading data results in an error, provide data as iterable reader (e.g. read_csv() method from Pandas with chunks). It may be necessary to use methods for initial data pre-processing like: e.g. DataFrame.dropna(), DataFrame.drop_duplicates(), DataFrame.sample().\n",
        "\n",
        "reader_full_data = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE)\n",
        "Batch size in rows."
      ],
      "metadata": {
        "id": "-2qXaw8mij38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_batch_rows = 2189\n"
      ],
      "metadata": {
        "id": "I-wcbaM0ipiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai.data_loaders import experiment as data_loaders\n",
        "from ibm_watsonx_ai.data_loaders.datasets import experiment as datasets\n",
        "\n",
        "dataset = datasets.ExperimentIterableDataset(\n",
        "    connection=training_data_references[0],\n",
        "    enable_sampling=False,\n",
        "    experiment_metadata=experiment_metadata,\n",
        "    number_of_batch_rows=number_of_batch_rows\n",
        "    )\n",
        "\n",
        "data_loader = data_loaders.ExperimentDataLoader(dataset=dataset)"
      ],
      "metadata": {
        "id": "Mlc4eQ2Eis1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue model training\n",
        "In this cell, the pipeline is incrementally fitted using data batches (via partial_fit calls).\n",
        "\n",
        "Note: If you need, you can evaluate the pipeline using custom holdout data. Provide the X_test, y_test and call scorer on them.\n",
        "\n",
        "Define scorer from the optimization metric\n",
        "This cell constructs the cell scorer based on the experiment metadata."
      ],
      "metadata": {
        "id": "e8St0thQi7P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import get_scorer\n",
        "\n",
        "scorer = get_scorer(experiment_metadata['scoring'])"
      ],
      "metadata": {
        "id": "NGDA6LVJi_Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the incremental learner\n",
        "For the best training performance set:\n",
        "\n",
        "n_jobs - to available number of CPUs."
      ],
      "metadata": {
        "id": "Wk7tA8iWjCvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model.steps[-1][1].impl.base_ensemble.set_params(n_jobs=CPU_NUMBER)"
      ],
      "metadata": {
        "id": "GbvIkwVCjGVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up a learning curve plot"
      ],
      "metadata": {
        "id": "20xgD-IbjKJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from ibm_watsonx_ai.utils.autoai.incremental import plot_learning_curve\n",
        "import time\n",
        "\n",
        "partial_fit_scores = []\n",
        "fit_times = []"
      ],
      "metadata": {
        "id": "yLT7rKcZjODL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit pipeline model in batches\n",
        "Tip: If the data passed to partial_fit is highly imbalanced (>1:10), please consider applying the sample_weight parameter:\n",
        "\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True,\n",
        "                                             sample_weight=compute_sample_weight('balanced', y_train))\n",
        "Note: If you have a holdout/test set please provide it for better pipeline evaluation and replace X_test and y_test in the following cell.\n",
        "\n",
        "from pandas import read_csv\n",
        "test_df = read_csv('DATA_PATH')\n",
        "\n",
        "X_test = test_df.drop([experiment_metadata['prediction_column']], axis=1).values\n",
        "y_test = test_df[experiment_metadata['prediction_column']].values\n",
        "If holdout set was not provided, 30% of first training batch would be used as holdout.\n",
        "\n",
        "Filter warnings for incremental training."
      ],
      "metadata": {
        "id": "P247izxOjUgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jrGsO-BkjYND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "for i, batch_df in enumerate(data_loader):\n",
        "    batch_df.dropna(subset=experiment_metadata[\"prediction_column\"], inplace=True)\n",
        "    X_train = batch_df.drop([experiment_metadata['prediction_column']], axis=1).values\n",
        "    y_train = batch_df[experiment_metadata['prediction_column']].values\n",
        "    if i==0:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3)\n",
        "    start_time = time.time()\n",
        "    pipeline_model = pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True)\n",
        "    fit_times.append(time.time() - start_time)\n",
        "    partial_fit_scores.append(scorer(pipeline_model, X_test, y_test))\n",
        "    plot_learning_curve(fig=fig, axes=axes, scores=partial_fit_scores, fit_times=fit_times)"
      ],
      "metadata": {
        "id": "1UsisGX5jlSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test pipeline model\n",
        "Test the fitted pipeline (predict)."
      ],
      "metadata": {
        "id": "9kiLayLNjn-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model.predict(X_test[:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "5MsV8vKUjqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the model\n",
        "In this section you will learn how to store the incrementally trained model."
      ],
      "metadata": {
        "id": "kYYdnMOJjyN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_metadata = {\n",
        "    client.repository.ModelMetaNames.NAME: 'P10 - Pretrained AutoAI pipeline'\n",
        "}\n",
        "\n",
        "stored_model_details = client.repository.store_model(model=pipeline_model, meta_props=model_metadata, experiment_metadata=experiment_metadata)"
      ],
      "metadata": {
        "id": "RYF4K92jj3jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the stored model details."
      ],
      "metadata": {
        "id": "v9b_F1g5j7tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stored_model_details"
      ],
      "metadata": {
        "id": "1SNTT-bvj-fu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}